{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer\n",
    "\n",
    "CountVectorizer converts a bunch of documents to vector so that we can use it with models. It basically just counts the number of times a particular word has occured. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 1 1 0 0 1 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 1 1]\n",
      " [1 1 0 1 1 1 1 0 1 0 0]]\n",
      "{u'and': 1, u'boy': 2, u'name': 6, u'is': 3, u'mayur': 4, u'am': 0, u'wohooo': 10, u'rock': 9, u'nice': 7, u'my': 5, u'pythonista': 8}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "docs = [\"Mayur is a nice boy.\", \"Mayur rock! wohooo!\", \"My name is Mayur, and I am a Pythonista!\"]\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(docs)\n",
    "print(X.todense())\n",
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DictVectorizer\n",
    "\n",
    "DictVectorizer will convert mappings to vectors. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  0.  2.  0.  0.  0.  1.  0.  0.]\n",
      " [ 1.  0.  1.  0.  2.  1.  2.  0.  3.  3.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "docs = [{\"Mayur\": 1, \"is\": 1, \"awesome\": 2}, {\"No\": 1, \"I\": 1, \"dont\": 2, \"wanna\": 3, \"fall\": 1, \"in\": 2, \"love\": 3}]\n",
    "dv = DictVectorizer()\n",
    "X = dv.fit_transform(docs)\n",
    "print(X.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfidfVectorizer\n",
    "\n",
    "In many text analytics applications, we need to convert the text into vectors to use with Machine Learning algorithms. This is known as the Vector Space Model. While CountVectorizer could be a solution, words like \"the\", \"a\", \"in\" etc. are common words and often are used in all kinds of documents. Using CountVectorizer gives more emphasis on such word counts which are not relevant. You could circumvent this problems by using `stop_words=\"english\"` which would filter out common words but let's say you have a different vocabulary, for instance a conversation between 2 Computer Science students would have words like \"RAM\", \"processor\", \"GPU\" mentioned too often and you'd have to manually add the stop words everytime for all the problems you solve. \n",
    "\n",
    "Thus in such scenarios, it is recommended to use `TfidfVectorizer` which will take care of such things. Every word is given a number according to the following formula:\n",
    "\n",
    "$$ \\text{tfidf }\\left(\\text{word}\\right)=\\text{tf}\\left(\\text{word},\\text{document}_i\\right)\\cdot\\text{idf}\\left(\\text{word}\\right) $$\n",
    "\n",
    "Where, \n",
    "1. tf(word, document_i) = Term Frequency of a word in the specific document i.\n",
    "2. idf(word) = Inverse Document Frequency of the word. \n",
    "\n",
    "Inverse Document Frequency is defined as the log of ratio of number of documents to the number of times the word has occured in the any document. \n",
    "\n",
    "$$ \\text{idf }\\left(w\\right)=\\log\\left(\\frac{n_d}{df\\left(w\\right)}\\right)$$\n",
    "\n",
    "Where, \n",
    "1. df(w) = number of times the word has occured in the any document. \n",
    "\n",
    "What is does intuitively is if a word has occured too many times in other document as well (common words like \"the\", \"is\") then it gives lesser weightage to such words in contrast to words that have occured more number of times in a single document in contrast to others. Which basically means that if a particular word occurs more number of times in a single document only, then it might be an important feature. \n",
    "\n",
    "Note that numerator and denominator are added with `1` to avoid underflow eg. when the document frequency is 0. \n",
    "\n",
    "Sklearn additionally also normalizes the output of tfidf to have a norm of 1. This is important since we're interested in similarities hence vectors like (1, 1) and (3, 3) are really the same (they go in same direction, just have different weights) which is achieved by dividing by the length of the vector.\n",
    "\n",
    "$$v_i=\\frac{v_i}{\\left|v\\right|_2}=\\frac{v_i}{\\sqrt{v_1^2+v_2^2+v_3^2+....+v_n^2}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.76749457  0.45329466  0.45329466  0.          0.        ]\n",
      " [ 0.          0.          0.45329466  0.45329466  0.76749457  0.        ]\n",
      " [ 0.6088451   0.          0.35959372  0.35959372  0.          0.6088451 ]]\n",
      "{u'also': 0, u'musician': 4, u'programmer': 5, u'is': 2, u'mayur': 3, u'guitarist': 1}\n",
      "[[0 1 1 1 0 0]\n",
      " [0 0 1 1 1 0]\n",
      " [1 0 1 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "cv_vectorizer = CountVectorizer()\n",
    "docs = [\"Mayur is a Guitarist\", \"Mayur is Musician\", \"Mayur is also a programmer\"]\n",
    "X_idf = tfidf_vectorizer.fit_transform(docs)\n",
    "X_cv = cv_vectorizer.fit_transform(docs)\n",
    "print(X_idf.todense())\n",
    "print(tfidf_vectorizer.vocabulary_)\n",
    "print(X_cv.todense())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the \"Mayur\" and \"is\" are given less weightage than \"guitarist\", \"musician\", \"programmer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
